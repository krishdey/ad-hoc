package com.myriad.aim;

import java.io.IOException;
import java.util.Collections;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.UUID;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.Cell;
import org.apache.hadoop.hbase.CellUtil;
import org.apache.hadoop.hbase.ZooKeeperConnectionException;
import org.apache.hadoop.hbase.client.Delete;
import org.apache.hadoop.hbase.client.Durability;
import org.apache.hadoop.hbase.client.Mutation;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.filter.Filter;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.Import;
import org.apache.hadoop.hbase.mapreduce.TableMapper;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.hbase.zookeeper.ZKClusterId;
import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.apache.zookeeper.KeeperException;

/**
 * Write table content out to files in hdfs.
 */
public class HashImporter extends TableMapper<ImmutableBytesWritable, Mutation> {
	private static final Log LOG = LogFactory.getLog(Import.class);
	final static String NAME = "import";
	public final static String TABLE_NAME = "import.table.name";
	public final static String WAL_DURABILITY = "import.wal.durability";
	private Map<byte[], byte[]> cfRenameMap;
	private List<UUID> clusterIds;
	private Filter filter;
	private Durability durability;

	/**
	 * @param row     The current table row key.
	 * @param value   The columns.
	 * @param context The current context.
	 * @throws IOException When something is broken with the data.
	 */
	@Override
	public void map(ImmutableBytesWritable row, Result value, Context context) throws IOException {
		try {
			writeResult(row, value, context);
		} catch (InterruptedException e) {
			e.printStackTrace();
		}
	}

	private void writeResult(ImmutableBytesWritable key, Result result, Context context)
			throws IOException, InterruptedException {
		Put put = null;
		Delete delete = null;
		if (LOG.isTraceEnabled()) {
			LOG.trace("Considering the row." + Bytes.toString(key.get(), key.getOffset(), key.getLength()));
		}
		if (filter == null || !filter.filterRowKey(key.get(), key.getOffset(), key.getLength())) {
			processKV(key, result, context, put, delete);
		}
	}

	protected void processKV(ImmutableBytesWritable key, Result result, Context context, Put put, Delete delete)
			throws IOException, InterruptedException {
		for (Cell kv : result.rawCells()) {
			kv = filterKv(filter, kv);
			// skip if we filter it out
			if (kv == null)
				continue;

			kv = convertKv(kv, cfRenameMap);
			// Deletes and Puts are gathered and written when finished
			/*
			 * If there are sequence of mutations and tombstones in an Export, and after
			 * Import the same sequence should be restored as it is. If we combine all
			 * Delete tombstones into single request then there is chance of ignoring few
			 * DeleteFamily tombstones, because if we submit multiple DeleteFamily
			 * tombstones in single Delete request then we are maintaining only newest in
			 * hbase table and ignoring other. Check - HBASE-12065
			 */
			if (CellUtil.isDeleteFamily(kv)) {
				Delete deleteFamily = new Delete(key.get());
				deleteFamily.addDeleteMarker(kv);
				if (durability != null) {
					deleteFamily.setDurability(durability);
				}
				deleteFamily.setClusterIds(clusterIds);
				context.write(key, deleteFamily);
			} else if (CellUtil.isDelete(kv)) {
				if (delete == null) {
					delete = new Delete(key.get());
				}
				delete.addDeleteMarker(kv);
			} else {
				if (put == null) {
					put = new Put(key.get());
				}
				addPutToKv(put, kv);
			}
		}
		if (put != null) {
			if (durability != null) {
				put.setDurability(durability);
			}
			put.setClusterIds(clusterIds);
			context.write(key, put);
		}
		if (delete != null) {
			if (durability != null) {
				delete.setDurability(durability);
			}
			delete.setClusterIds(clusterIds);
			context.write(key, delete);
		}
	}

	protected void addPutToKv(Put put, Cell kv) throws IOException {
		put.add(kv);
	}

	@Override
	public void setup(Context context) {
		Configuration conf = context.getConfiguration();
		cfRenameMap = createCfRenameMap(conf);
		filter = instantiateFilter(conf);
		String durabilityStr = conf.get(WAL_DURABILITY);
		if (durabilityStr != null) {
			durability = Durability.valueOf(durabilityStr.toUpperCase(Locale.ROOT));
		}
		// TODO: This is kind of ugly doing setup of ZKW just to read the clusterid.
		ZooKeeperWatcher zkw = null;
		Exception ex = null;
		try {
			zkw = new ZooKeeperWatcher(conf, context.getTaskAttemptID().toString(), null);
			clusterIds = Collections.singletonList(ZKClusterId.getUUIDForCluster(zkw));
		} catch (ZooKeeperConnectionException e) {
			ex = e;
			LOG.error("Problem connecting to ZooKeper during task setup", e);
		} catch (KeeperException e) {
			ex = e;
			LOG.error("Problem reading ZooKeeper data during task setup", e);
		} catch (IOException e) {
			ex = e;
			LOG.error("Problem setting up task", e);
		} finally {
			if (zkw != null)
				zkw.close();
		}
		if (clusterIds == null) {
			// exit early if setup fails
			throw new RuntimeException(ex);
		}
	}
}