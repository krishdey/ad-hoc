package com.myriad.aim;

import java.io.IOException;
import java.util.Collections;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.UUID;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.Cell;
import org.apache.hadoop.hbase.ZooKeeperConnectionException;
import org.apache.hadoop.hbase.client.Delete;
import org.apache.hadoop.hbase.client.Durability;
import org.apache.hadoop.hbase.client.Mutation;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.filter.Filter;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.Import;
import org.apache.hadoop.hbase.mapreduce.TableMapper;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.hbase.zookeeper.ZKClusterId;
import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
import org.apache.zookeeper.KeeperException;

/**
 * Write table content out to files in hdfs.
 */
public class HashImporter extends TableMapper<ImmutableBytesWritable, Mutation> {
	private static final Log LOG = LogFactory.getLog(Import.class);
	final static String NAME = "import";
	public final static String TABLE_NAME = "import.table.name";
	public final static String WAL_DURABILITY = "import.wal.durability";
	private List<UUID> clusterIds;
	private Durability durability;

	/**
	 * @param row     The current table row key.
	 * @param value   The columns.
	 * @param context The current context.
	 * @throws IOException When something is broken with the data.
	 */
	@Override
	public void map(ImmutableBytesWritable row, Result value, Context context) throws IOException {
		try {
			writeResult(row, value, context);
		} catch (InterruptedException e) {
			e.printStackTrace();
		}
	}

	private void writeResult(ImmutableBytesWritable key, Result result, Context context)
			throws IOException, InterruptedException {
		Put put = null;
		Delete delete = null;
		if (LOG.isTraceEnabled()) {
			LOG.trace("Considering the row." + Bytes.toString(key.get(), key.getOffset(), key.getLength()));
		}
		processKV(key, result, context, put, delete);
	}

	protected void processKV(ImmutableBytesWritable key, Result result, Context context, Put put, Delete delete)
			throws IOException, InterruptedException {
		for (Cell kv : result.rawCells()) {
			if (put == null) {
				put = new Put(key.get());
			}
			addPutToKv(put, kv);

			if (put != null) {
				if (durability != null) {
					put.setDurability(durability);
				}
				put.setClusterIds(clusterIds);
				context.write(key, put);
			}
		}
	}

	protected void addPutToKv(Put put, Cell kv) throws IOException {
		put.add(kv);
	}

	@Override
	public void setup(Context context) {
		Configuration conf = context.getConfiguration();
		String durabilityStr = conf.get(WAL_DURABILITY);
		if (durabilityStr != null) {
			durability = Durability.valueOf(durabilityStr.toUpperCase(Locale.ROOT));
		}
		// TODO: This is kind of ugly doing setup of ZKW just to read the clusterid.
		ZooKeeperWatcher zkw = null;
		Exception ex = null;
		try {
			zkw = new ZooKeeperWatcher(conf, context.getTaskAttemptID().toString(), null);
			clusterIds = Collections.singletonList(ZKClusterId.getUUIDForCluster(zkw));
		} catch (ZooKeeperConnectionException e) {
			ex = e;
			LOG.error("Problem connecting to ZooKeper during task setup", e);
		} catch (KeeperException e) {
			ex = e;
			LOG.error("Problem reading ZooKeeper data during task setup", e);
		} catch (IOException e) {
			ex = e;
			LOG.error("Problem setting up task", e);
		} finally {
			if (zkw != null)
				zkw.close();
		}
		if (clusterIds == null) {
			// exit early if setup fails
			throw new RuntimeException(ex);
		}
	}
}